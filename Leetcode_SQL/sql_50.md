### 1193. Monthly Transactions I
``` mysql
SELECT SUBSTR(trans_date,1,7) AS month,
        country,
        COUNT(id) AS trans_count,
        SUM(IF(state = "approved",1 ,0)) AS approved_count,
        SUM(amount) AS trans_total_amount,
        SUM(IF(state = "approved",amount ,0)) AS approved_total_amount
FROM Transactions
GROUP BY 1, 2
```

**æå–æ—¥æœŸçš„å¹´ï¼Œæœˆï¼š**
æ–¹æ³•1ï¼šSUBSTR(trans_date,1,7) 
æ–¹æ³•2ï¼šleft(trans_date,7)
æ–¹æ³•3ï¼šDATE_FORMAT(trans_date, '%Y-%m')
**count å†…å®¹ä¸ºapprovedçš„æ•°é‡**
ç”¨SUMå’ŒIF,IFåˆ¤æ–­å†…å®¹ä¸ºapprovedï¼Œå°±è®¾ç½®ä¸º1ï¼Œtotal amountä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå¦‚æœä¸ºapprovedå°±è®¾ç½®ä¸ºamountã€‚

### 1174. Immediate Food Delivery II
``` mysql
SELECT ROUND(AVG(order_date = customer_pref_delivery_date)*100, 2) AS immediate_percentage
FROM Delivery
WHERE  (customer_id, order_date) IN(
                                SELECT customer_id,min(order_date)
                                FROM Delivery
                                GROUP BY 1
                                )
```
**æ€è·¯**: æˆ‘ä»¬è¦åœ¨æ»¡è¶³ç‰¹å®šæ¡ä»¶çš„æ•°æ®ä¸­ï¼Œå†è¿›è¡Œä¸€æ¬¡åˆ¤æ–­ï¼Œæœ€ç®€ä¾¿çš„æ–¹æ³•å°±æ˜¯åœ¨whereé‡Œé¢æŠŠç‰¹å®šæ¡ä»¶ç­›é€‰å‡ºæ¥ã€‚ä¸Šé¢è¿™ä¸€é¢˜ç”¨äº†subquaryå’Œinæ¥ç­›é€‰å‡ºç‰¹å®šæ¡ä»¶çš„rowsã€‚

### 550. Game Play Analysis IV
``` mysql
SELECT ROUND(COUNT(DISTINCT player_id)/(SELECT COUNT(DISTINCT player_id) 
                                            FROM Activity) 
            , 2) AS fraction
FROM Activity
WHERE (player_id, DATE_SUB(event_date, INTERVAL 1 DAY)) IN(
                    SELECT player_id, MIN(event_date)
                    FROM Activity
                    GROUP BY 1)
```
**tips:**SELECTé‡Œé¢ç”¨ä¸€ä¸ªsubquaryï¼Œè¿™æ ·å¯ä»¥è·å–ä¸¤å¼ è¡¨é‡Œçš„æ•°æ®ã€‚
**æ—¥æœŸéš”ä¸€å¤©çš„å†™æ³•**ï¼šDATE_SUB(event_date, INTERVAL 1 DAY)
**æ–¹æ³•2ï¼šç”¨Join**
``` mysql
SELECT ROUND(COUNT(t2.player_id)/COUNT(t1.player_id),2) AS fraction
FROM
        (SELECT player_id, MIN(event_date) AS first_login 
        FROM Activity 
        GROUP BY player_id) t1 
LEFT JOIN Activity t2
        ON t1.player_id = t2.player_id 
        AND t1.first_login = t2.event_date - 1
```
**tips**: ä¸€å¼ è¡¨é‡Œæ˜¯ç¬¬ä¸€å¤©ç™»å½•çš„rowï¼Œç¬¬äºŒå¼ è¡¨é‡Œæ˜¯é¦–æ¬¡ç™»é™†çš„ç¬¬äºŒå¤©çš„rowï¼Œå†é€šLEFT joinåˆèµ·æ¥ï¼Œselecté‡Œçš„å†…å®¹å’Œä¸Šé¢ä¸€ä¸ªæ–¹æ³•ä¸€ä¸ªæ„æ€ã€‚

### Interview Query July 11ï¼ŒExam Scores
é¢˜ç›®æ˜¯ä¸€å¼ è¡¨æ ¼æ˜¯æ¯ä¸ªå­¦ç”Ÿå’Œå››ä¸ªè€ƒè¯•æˆç»©ï¼Œcolumn 1 student_id, column 2 student name, column 3, exam 1 score... ç„¶åexam 2, 3, 4.è¦æ±‚æ˜¯è½¬åŒ–æˆä¸€ä¸ªå­¦ç”Ÿå’Œä»–çš„å››ä¸ªæˆç»©ä¸ºä¸€rowã€‚
``` mysql
SELECT student_name,
        SUM(CASE WHEN exam_id = 1 THEN score ELSE NULL END) AS exam_1,
        SUM(CASE WHEN exam_id = 2 THEN score ELSE NULL END) AS exam_2,
        SUM(CASE WHEN exam_id = 3 THEN score ELSE NULL END) AS exam_3,
        SUM(CASE WHEN exam_id = 4 THEN score ELSE NULL END) AS exam_4
FROM exam_scores
GROUP BY student_id;
```
ä»¥ä¸Šçš„æ–¹æ³•ä½¿ç”¨äº†case whenï¼Œå› ä¸ºéœ€è¦æ¯ä¸ªå­¦ç”Ÿåœ¨ä¸€ä¸ªrowé‡Œé¢ï¼Œæ‰€ä»¥ç”¨äº†group byï¼Œè€Œä¸ºäº†ç”¨group byï¼Œåœ¨selecté‡Œé¢å°±éœ€è¦ç”¨sumã€‚
ä»¥ä¸‹æ˜¯ä½¿ç”¨ifçš„æ–¹æ³•ï¼Œå…¶ä»–é€»è¾‘ä¸å˜ã€‚
``` mysql
SELECT student_name,
  SUM(IF(exam_id=1, score, NULL)) AS exam_1,
  SUM(IF(exam_id=2, score, NULL)) AS exam_2,
  SUM(IF(exam_id=3, score, NULL)) AS exam_3,
  SUM(IF(exam_id=4, score, NULL)) AS exam_4
FROM exam_scores
GROUP BY student_id;
```
### Interview Query Pre-Launching Shows
Letâ€™s say that you are working as a data scientist at Amazon Prime Video, and they are about to launch a new show, but first want to test the launch on only 10,000 customers first

How do we go about selecting the best 10,000 customers for the pre-launch?

What would the process look like for pre-launching the TV show on Amazon Prime to measure how it performs?
**Answer:**
To select the best 10,000 customers, the first things we wanna do is to recognizing our goals. 
**Step 1 recognizing our goals**
    1. Test the performance of the new show, eg.user engagement, viewer number, tecnical performance...
    2. estimate the impact of the show of the whole amazon prime users.
   
**Step 2 customer segmentation**
    1. If our goal is to test the performance of the new show, to get sufficient data, we can use historical data and choose the group people who are the target customer of this type of show. Within such group of people, we would like to randomly select from the whole group to avoid any bias.
    2. If our goal is to estimate the impact on whole prime users, we can use Strategy Sampling to make sure the sample could represent the who population.
   
**Step 3 A/B testing**
    1. after selecting our sample, we could conduct a A/B test, we would divide the 10,000 selected customers into control and test groups, and we will make sure the two group are independent.
   
**Step 4 metrics**
We would measure KPIs such as viewership numbers, engagement rates and technical performance. We could also conduct some survey to get some feedback.To compare the key performance indicators (KPIs) between the test group and the control group, we would use statistical tests such as z-test or t-test. These tests help us determine if there are significant differences in the metrics, providing insights into the show's potential success.

Define Comparison Method:
We would employ z-test or t-test to compare the KPIs such as viewership numbers, engagement rates, customer feedback, and technical performance between the test and control groups.

    Z-test: This test is suitable when we have a large sample size (n > 30) and the population variance is known. It helps in comparing the means of two groups.
    T-test: This test is used when the sample size is smaller (n <= 30) or the population variance is unknown. It also compares the means of two groups but is more appropriate for smaller samples.

### Interview Query Empty Neighborhoods
![alt text](image.png)

``` mysql
SELECT name
FROM neighborhoods
WHERE id NOT IN (
    SELECT neighborhood_id
    FROM users
)
```
**TIPS**:ä¸Šé¢çš„æ–¹æ³•ä¼šå¥½ç†è§£ï¼Œä¸¤å¼ è¡¨ç”¨NOT INå’Œä¸€ä¸ªsubqueryé€‰å‡ºå‡ºç°åœ¨ä¸€å¼ å›¾è€Œä¸å†ç¬¬äºŒå¼ å›¾çš„rowï¼Œä½†æ˜¯ç”±äºè¦queryä¸€è¾¹æ‰¾å‡ºæ‰€æœ‰çš„idï¼Œæ‰€ä»¥ä¸å¤Ÿæœ‰æ•ˆç‡ã€‚ä»¥ä¸‹çš„æ–¹æ³•æ˜¯ç”¨left joinï¼Œåº”è¯¥æ˜¯ä¸€ä¸ªæ›´efficientçš„æ–¹æ³•ã€‚
``` mysql
SELECT n.name   
FROM neighborhoods AS n 
LEFT JOIN users AS u
    ON n.id = u.neighborhood_id
WHERE u.id IS NULL
```
LEFT JOINä¼šä¿ç•™æ‰€æœ‰çš„å·¦è¾¹è¡¨çš„rowï¼Œåœ¨è¿™é‡Œä¹Ÿå°±æ˜¯neighborhoodsã€‚ç„¶åæœ€åç”¨where IS NULLæ¥ç­›é€‰å‡ºæ²¡æœ‰å‡ºç°åœ¨å³è¾¹è¡¨é‡Œçš„rowã€‚

### Interview Query Good Grades and Favorite Colors
![alt text](image-1.png)
``` python
import pandas as pd

def grades_colors(students_df):
    students_df = students_df[(students_df['grade']>90) &
        students_df['favorite_color'].isin(['red', 'green'])]
    return students_df
```
**tips**:def å’Œ returnä¸€èµ·ï¼Œ[]è¡¨ç¤ºdfé‡Œå–ä¸€éƒ¨åˆ†ï¼Œ()å¯ä»¥è¯´æ˜andçš„ä¼˜å…ˆçº§ï¼Œä¹Ÿæ˜¯isin()è°ƒç”¨functionã€‚

### Interview Query Monthly Customer Report
![alt text](image-2.png)
``` mysql
SELECT MONTH(created_at) AS month,
    COUNT(DISTINCT user_id) AS num_customers,
    COUNT(DISTINCT t.id) AS num_orders,
    SUM(quantity*price) AS order_amt
FROM transactions t
LEFT JOIN products p
    ON t.product_id = p.id
WHERE YEAR(created_at) ='2020'
GROUP BY 1
```
### Interview Query Closest SAT Scores
![alt text](image-4.png)
``` mysql
SELECT s1.student AS one_student,
    s2.student AS other_student,
    ABS(s1.score - s2.score) AS score_diff
FROM scores s1
INNER JOIN scores s2
    WHERE s1.id != s2.id
    AND s1.id < s2.id
ORDER BY 3, 1
LIMIT 1
```
**å¦‚ä½•cross join**
åœ¨è¿™é“é¢˜é‡Œé¢ï¼Œæˆ‘ä»¬è¦æ‰¾æˆç»©ç›¸å·®æœ€å°çš„ä¸¤ä¸ªå­¦ç”Ÿï¼Œæ‰€ä»¥æˆ‘ä»¬å°†æ‰€æœ‰å­¦ç”Ÿä¸¤ä¸¤é…å¯¹ã€‚INNER JOIN scores s2 WHERE s1.id != s2.id
ç¬¬äºŒä¸ªé‡ç‚¹ï¼Œå› ä¸ºç”¨ä¸Šé¢è¿™ä¸ªæ–¹æ³•åŒæ ·ä¸¤ä¸ªäººä¼šå‡ºç°ä¸¤éï¼Œæ‰€ä»¥ç”¨AND s1.id < s2.idæ¥ç­›é€‰å‡ºç¬¬ä¸€ç»„ï¼Œæ¶ˆé™¤é‡å¤

### Interview Query Estimating D
**Question**ï¼šGiven ğ‘ samples from a uniform distribution [0,ğ‘‘], how would you estimate ğ‘‘?
**Answer** Because it's uniformly distribute, average of all the sample would close to the half of d, bigger sample size, more accurate result. Thus, the answer would be 2*AVG(n)


### Interview Query Employee Salaries
![alt text](image-5.png)
``` mysql
SELECT name AS department_name,
        AVG(CASE WHEN salary > 100000
            THEN 1 ELSE 0 END) AS percentage_over_100k,
        COUNT(DISTINCT e.id) AS number_of_employees
FROM departments d
LEFT JOIN employees e
    ON d.id = e.department_id
GROUP BY 1
HAVING COUNT(DISTINCT e.id) >= 10
ORDER BY 2 DESC
LIMIT 3
```
**tip:**æœ€é‡è¦çš„ä¸€ä¸ªï¼Œå½“è¦ç®—percentageçš„æ—¶å€™ï¼Œä¸€å®šè¦æƒ³åˆ°case whenï¼Œç”¨AVGï¼ˆ0ï¼Œ1ï¼‰æ¥è®¡ç®—percentageã€‚
è¿™é‡Œç­›é€‰idäººæ•°å¤§äº10æ—¶ç”¨äº†countï¼Œæ‰€ä»¥å¿…é¡»åœ¨groupåé¢ï¼Œæ‰€ä»¥è¦ç”¨havingã€‚

### Interview Query Employee Salaries
![alt text](image-6.png)
``` mysql
SELECT name, IFNULL(SUM(distance),0) AS distance_traveled
FROM users u
LEFT JOIN rides r
    ON u.id = r.passenger_user_id
GROUP BY u.id
ORDER BY 2 DESC
```
**tips**å¯ä»¥æ³¨æ„ä¸€ä¸‹è¿™ç§é—®é¢˜æ€ä¹ˆå¤„ç†0ï¼ŒæŠŠnullå˜æˆ0ä¼šæ›´å‡†ç¡®ã€‚

### 1633.Percentage of Users Attended a Contest
``` mysql
SELECT contest_id,
        ROUND(COUNT(user_id)/(SELECT COUNT(user_id)
                        FROM Users)*100, 2) AS percentage
FROM Register
GROUP BY 1
ORDER BY 2 DESC, 1
```
**pythonæ–¹æ³•**
``` python
total_users = users["user_id"].unique()

register_grouped = (
    register.groupby("contest_id")["user_id"]
    .nunique()
    .reset_index(name="count_unique_users")
)

register_grouped["percentage"] = (
    register_grouped["count_unique_users"] / total_users
) * 100

register_grouped["percentage"] = register_grouped["percentage"].round(2)
```
å°æ³¨æ„äº‹é¡¹ï¼Œè¡¨é‡Œçš„column nameè¦åŠ å¼•å·ã€‚

### 1211. Queries Quality and Percentage
``` mysql
SELECT query_name, 
        ROUND(AVG(rating/position),2) AS quality,
        ROUND(AVG(CASE WHEN rating < 3 THEN 1 ELSE 0 END)*100,2) AS poor_query_percentage
FROM Queries
GROUP BY query_name
```
**æ–¹æ³•2ï¼šç”¨ window function**
``` mysql
SELECT DISTINCT query_name,
        ROUND(AVG(rating/position) OVER(PARTITION BY query_name), 2) AS quality,
        ROUND(AVG(CASE WHEN rating < 3 THEN 1 ELSE 0 END) OVER(PARTITION BY query_name)*100, 2) AS poor_query_percentage
FROM Queries
```
### 2356. Number of Unique Subjects Taught by Each Teacher
![alt text](image-7.png)
``` mysql
SELECT teacher_id, COUNT(DISTINCT subject_id) AS cnt
FROM Teacher
GROUP BY 1
```
æ¯”è¾ƒç®€å•çš„ä¸€é“sqlé¢˜ï¼Œå¯ä»¥ä¸»è¦ç»ƒä¹ ä¸€ä¸‹pandasåšæ³•ã€‚
``` python
teacher_grouped = (
    teacher.groupby("teacher_id")[subject_id]
    .unique()
    .reset_index(name = "cnt")
)
```
**.numique**å‡ºæ¥æ˜¯countçš„ç»“æœï¼Œç›´æ¥**unique**å‡ºæ¥æ˜¯ç»™ä¸€ä¸ªlistï¼ŒåŒ…å«äº†uniqueçš„ç»“æœã€‚
ä¸‹é¢æ˜¯åŒ…è£…æˆfunctionçš„å†™æ³•ï¼š
``` python
def count_unique_subjects(teacher: pd.DataFrame) -> pd.DataFrame:
    # Group by teacher_id and count the number of unique subject_ids
    result = teacher.groupby('teacher_id')['subject_id'].nunique().reset_index()
    result.rename(columns={'subject_id': 'cnt'}, inplace=True)
    
    return result
```
OR
``` python
def count_unique_subjects(teacher: pd.DataFrame) -> pd.DataFrame:
    return teacher.groupby('teacher_id')['subject_id'].nunique().reset_index(name='cnt')
```

### 1141. User Activity for the Past 30 Days I
![alt text](image-8.png)
``` mysql
SELECT activity_date AS day, COUNT(DISTINCT user_id) AS active_users
FROM Activity
WHERE activity_date BETWEEN '2019-06-28' AND '2019-07-27'
GROUP BY 1
```
æ—¥æœŸ30å¤©çš„å¦ä¸€ç§å†™æ³•ï¼šBETWEEN date_add('2019-07-27' , INTERVAL -29 DAY) and '2019-07-27'
``` python
import pandas as pd

def user_activity(activity: pd.DataFrame) -> pd.DataFrame:

    return (activity[activity
                    .activity_date.between("2019-06-28", "2019-07-27")]

                    .rename(columns = {'activity_date': 'day',
                                       'user_id': 'active_users'})

                    .groupby('day')['active_users']
                    .nunique().reset_index())
```

### 1070. Product Sales Analysis III
``` mysql
SELECT product_id, year AS first_year, quantity, price
FROM Sales
WHERE (product_id, year) IN
                    (SELECT product_id, MIN(year) AS year
                    FROM Sales
                    GROUP BY 1)
```
